%!/usr/bin/env pdflatex
%-*- coding: utf-8 -*-
%@author : Romain Graux
%@date : 2022 June 07, 11:28:34
%@last modified : 2022 June 17, 11:30:02


Some improvements can be made to obtain better results, performance and/or robustness. 

\section{Nucleotide rate and distortion}

As seen in the performance section~\ref{sec:performance}, the model suffers from limitations on the minimum distortion achievable. This is mostly due to the linear quantization which is the only lossy part of the algorithm, so several refinements are possible to get better results. The main problem is that the latent representation $y$ is definitely non uniform, it is thus non optimal to divide the space in equally long gaps, instead we would have to divide the space in equally probable gaps. So in our case we would have to construct small gaps around $0$ and bigger ones at the ends so that each one is equiprobable regarding the actual $y$ distribution. 
For that purpose, it is possible to assume a normal distribution and instead of keeping track of the minimum and maximum value of $y$ in order to quantize and dequantize, we would have to track the mean and the standard deviation of $y$ so we can shift it exactly around $0$ and control the spread of the distribution to control the nucleotide rate. One advantage is that we would have negative and positive values around $0$, so we would give small codewords for the most frequent $y$ value, even for the negative values. It would impact the final nucleotide rate in a good way.

The easier way of actually taking into account the distribution and quantize it accordingly is to quantize the latent representation with the entropy model used in the standard model that has learned all parameters from the distribution, it would give us better quantized symbols. But then, we would have to apply the algorithm slice per slice as it is the case with the classical model instead of directly encode the full latent representation. It would give us better rate over distortion results because it has been trained directly on that metric. And we would have the already trained models for different bitrates to control the rate-distortion ratio.

In this algorithm we used the JPEG DNA codec that is especially built to deal with visual information with all the JPEG parts (DCT, quantization tables, ...) in order to encode the DC and AC coefficients. 
Therefore it would be a great idea to build a general purpose coefficient coder instead of zigzaging the $8 \times 8$ blocks and encode the first value as DC coefficient and the $63$ next as AC coefficients. In our case, it is unlikely to have a lot of following zeros, which is not a problem since the AC coder is traditionally encoding the value with its category and its corresponding codeword if no $0$ are encountered.
But the first DC value of each block are not necessarily close between adjacent blocks as it is the case for visual image, so instead of encoding the difference between adjacent blocks, we could encode the value like the $63$ following AC coefficients.
It would thus worth to encode the full flatten input array instead of diving it into $8 \times 8$ blocks. 


\section{Simulation robustness}

As we saw in the simulation section~\ref{sec:simu}, the model is not thoroughly robust against errors in the DNA stream, due to the JPEG DNA codec behavior. Without changing the whole structure, we can adapt the codec so that there is no categories anymore. This would allow us to reconstruct the final array even with errors introduced, it would directly affect the coefficient values but it would be almost always possible to decode the value. The problem with this technique is that, to use the PAIRCODE algorithm, we would have to choose a codebook with constant length codewords to encode all the coefficients, which would directly affect the nucleotide rate. But it can be a tradeoff to choose at compression time, we encode all coefficients from the same codebook that will ensure reconstruction while increasing the nucleotide rate.

Another approach would be to train a learning based model with the same loss taking into account the distortion and the nucleotide rate but simulating the compressed DNA representation by simply mapping a nucleotide every $2$ bits from the entropy model of the standard model. It would allow the model to learn how to compress the point clouds so that the compressed representation is as little error-prone as possible during the sequencing, storage and synthesis.
To build such a model, we need a simulator that keeps track of the gradients but currently, the simulator is built on top of numpy which is not suitable for gradient learning models. 
